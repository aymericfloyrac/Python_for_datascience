{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floyrac Aymeric et Ané Benoit, Projet Python 2A ENSAE ParisTech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.jpg\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection d'anomalies cardiaques à partir d'enregistrements sonores de battements de coeur par phonocardiogramme\n",
    "\n",
    "Notre projet Python provient du Challenge Physionet 2016 (https://physionet.org/). Ce site propose un challenge tous les ans sur le thème de l'analyse de signaux physiologiques en OpenSource. En 2016, un ensemble d'enregistrements du coeur par phonocardiogramme ont été fournis aux candidats classés dans 6 sous dossiers (a,b,c,d,e et f) afin de construire un modèle prédictif pour déterminer si un coeur bat de façon anormale ou non. Notre objectif était de construire un ensemble de variables explicatives à partir d'une base constituée des signaux bruts afin de bâtir un modèle de Machine Learning. Au préalable, un travail de recherche bibliographique a été necessaire afin de se familiariser avec le vocabulaire du sujet, trouver la méthodologie à appliquer, connaitre les features à extraire ainsi que les modèles ayant déjà été appliqués dans des cas similaires. L'ensemble des articles que nous avons consultés sont renseignés en fin de projet dans la bibliographie.  \n",
    "\n",
    "Nous pouvons présenter notre travail à travers 5 étapes :  \n",
    "\n",
    "1) La conversion des enregistrements sonores en matrice exploitable  \n",
    "2) Notre algorithme qui nous permet de traiter le signal brut pour rendre l'extraction des features plus efficace  \n",
    "3) la visualisation des données  \n",
    "4) L'analyse des features  \n",
    "5) les modèles prédictifs de Machine Learning  \n",
    "6) Nos commentaires sur le travail effectué  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des enregistrements sonores en matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette partie de code est de convertir les enregistrements sonores en matrice exploitable pour notre algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wave, os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###On importe chaque .wav pour le transformer en vecteur. Nous bouclons pour chaque fichier .wav se trouvant dans un dossier\n",
    "\n",
    "zero_a = []\n",
    "zero_a = pd.DataFrame(zero_a)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-a'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_a = zero_a.append(df)\n",
    "    del data, fs\n",
    "\n",
    "zero_b = []\n",
    "zero_b = pd.DataFrame(zero_b)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-b'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_b = zero_b.append(df)\n",
    "    del data, fs\n",
    "\n",
    "zero_c = []\n",
    "zero_c = pd.DataFrame(zero_c)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-c'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_c = zero_c.append(df)\n",
    "    del data, fs\n",
    "\n",
    "zero_d = []\n",
    "zero_d = pd.DataFrame(zero_d)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-d'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_d = zero_d.append(df)\n",
    "    del data, fs\n",
    "    \n",
    "zero_e = []\n",
    "zero_e = pd.DataFrame(zero_e)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-e'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_e = zero_e.append(df)\n",
    "    del data, fs\n",
    "\n",
    "zero_f = []\n",
    "zero_f = pd.DataFrame(zero_f)\n",
    "path = 'C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-f'\n",
    "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
    "    fs, data = wavfile.read(filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    zero_f = zero_f.append(df)\n",
    "    del data, fs\n",
    "    \n",
    "train = []\n",
    "train = pd.DataFrame(train)\n",
    "train = zero_a.append(zero_b, ignore_index=True)\n",
    "train = train.append(zero_e, ignore_index=True)\n",
    "\n",
    "test = []\n",
    "test = pd.DataFrame(test)\n",
    "test = zero_c.append(zero_d, ignore_index=True)\n",
    "test = test.append(zero_f, ignore_index=True)\n",
    "\n",
    "#Nous exportons les bases au format .npy\n",
    "np.save('training_ab.npy', train, allow_pickle=True)\n",
    "np.save('test_cdf.npy', test, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation de la base\n",
    "Nous utilisons ici les matrices au format .npy afin d'éviter d'importer des csv et de travailler avec des dataframes. Cette méthode s'est avérée utile dans la mesure où nos ordinateurs travaillaient plus vite avec ces formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import wave, os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import find_peaks, welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fftpack import fft,fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = np.load('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\temp\\\\training_ab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.load('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\temp\\\\test_cdf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons la variables à prédire (car d'un coeur sain ou non) pour chacun des training sets disponibles que nous convertissons en vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training : a b d f\n",
    "labelsa = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-a/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "labelsb = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-b/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "labelsc = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-c/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "labelsd = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-d/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "labelse = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-e/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "labelsf = pd.read_csv('C:\\\\Users\\\\benoit\\\\Desktop\\\\ENS\\\\ENSAE\\\\2A\\\\S1\\\\Python\\\\Projet\\\\training\\\\training-f/REFERENCE.csv',header=None,names=['filename','label'])['label']\n",
    "\n",
    "labels_training = []\n",
    "labels_training = pd.DataFrame(labels_training)\n",
    "labels_training = labelsa.append(labelsb, ignore_index=True)\n",
    "labels_training = labels_training.append(labelse, ignore_index=True)\n",
    "labels_training = np.asarray(labels_training)\n",
    "labels_test = []\n",
    "labels_test = pd.DataFrame(labels_test)\n",
    "labels_test = labelsc.append(labelsd, ignore_index=True)\n",
    "labels_test = labels_test.append(labelsf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_training = np.asarray(labels_training)\n",
    "labels_test = np.asarray(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(labels_training.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent procéder à la construction de notre algorithme. Nous avons utilisé une classe faisant appel à plusieurs fonctions nous permettant d'aboutir à un vecteur de features que nous pourons par la suite exploiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons appris que les battements d'un coeur se décomposent en 4 phases et dans l'odre suivant : S1, systole, S2 et diastole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nous créons la liste permettant de\n",
    "phases_list = ['S1','S2','systole','diastole']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'image suivante présente les différentes phases du battement du coeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"battement.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan de la class PCG\n",
    "Nous allons vus présenter, en plus des remarques incorporer dans le code, l'ensemble des fonctions que nous avons crée :  \n",
    "\n",
    "1) init : Normalisation du signal considéré  \n",
    "2) envelogram : Calcul de l'envelogramme du signal auquel on applique un seuil pour débruiter le signal. Nous cherchons à calculer l'envelogramme à partir. La détermination de cette nouvelle courbe se fait à partir de l'énergie de Shannon. La formule est également disponible ci-dessous.  \n",
    "\n",
    "Le graphique suivant nous permet d'illustrer l'envelogramme ce que nous cherchons à calculer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"E.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"enve.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) consec_count : On compte le nombre de fois qu'il y a une répétition d'une même valeur. Cette fonction sera déterminante pour segmenter les phases cardiaques  \n",
    "4) segmentation : On segmente le signal en différente phase. Chacune ayant leurs caractéristiques\n",
    "5) timelengths : On détermine la durée de chaque phase  \n",
    "6) timefeatures : On extrait les features de durée des phases  \n",
    "7) computeborne : On détermine les bornes de chaque phase dans le vecteur total afin de pouvoir calculer des valeurs sur chaque phase du signal  \n",
    "8) moments : On détermine pour chaque phase le kurtosis et l'asymétrie du signal  \n",
    "9) computeMFCC : \"Mel-frequency cepstral coefficients\" sont dérivés de l'analyse spectral d'un signal audio  \n",
    "10) extractFeatures : On extrait l'ensemble de nos features calculés à partir des fonctions précédentes dans un vecteur par signal  \n",
    "\n",
    "Des notes sont présentes dans le code pour accompagner le lecteur dans sa compréhension du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PCG:\n",
    "    \n",
    "    def __init__(self,signal):\n",
    "        self.signal = np.asarray(signal[~np.isnan(signal)])-np.mean(signal[~np.isnan(signal)]) #on ignore les NaN et on recentre le signal\n",
    "        self.normalized_signal = self.signal/np.std(self.signal)\n",
    "        self.fs = 2000\n",
    "        \n",
    "    def envelogram(self):\n",
    "        E = self.signal\n",
    "        #on remplace les 0 par des petites valeurs (on doit éviter d'avoir un 0 pour calculer les log)\n",
    "        for i in range(self.signal.shape[0]):\n",
    "            if E[i]==0:\n",
    "                E[i] = E[i-1]/10\n",
    "        E = E/np.max(np.abs(E))\n",
    "        #E = E/np.std(E)\n",
    "        E = -1*(E**2) * np.log(E**2)\n",
    "\n",
    "        #on met les petites valeurs à 0 \n",
    "        # il faut d'abord calculer les maximums locaux \n",
    "        window_length = 750\n",
    "        local_max = np.array([np.max(E[i-window_length:i+window_length]) for i in range(window_length,len(E)-window_length)])\n",
    "        local_max = np.concatenate((local_max[0]*np.ones(window_length),local_max,local_max[-1]*np.ones(window_length)))\n",
    "        s = np.std(E)\n",
    "        m = np.mean(E)\n",
    "        mask = E>local_max/5\n",
    "        mask2 = np.abs(E-m)>s/2\n",
    "        mask3 = E>np.mean(E)*2\n",
    "        return E*mask\n",
    "    \n",
    "    \n",
    "    def consec_count(self):\n",
    "        e = self.envelogram()\n",
    "        start = 0\n",
    "        count_mask = []\n",
    "        while  start < len(e): \n",
    "            i = start\n",
    "            current = e[start]\n",
    "            while e[i]==current and i < len(e)-1:\n",
    "                i=i+1\n",
    "            l = int(i - start + 1)\n",
    "            count_mask = np.concatenate((count_mask,l*[l]))\n",
    "            start = i + 1\n",
    "        return count_mask\n",
    "    \n",
    "    def segmentation(self):\n",
    "        E = self.envelogram()\n",
    "        count_vec = self.consec_count() # le vecteur de comptage associé\n",
    "        length_threshold = 50\n",
    "        beat_mask = count_vec<=length_threshold\n",
    "        systole_mask = count_vec>length_threshold\n",
    "        segmentation = np.asarray([])\n",
    "        mean_beat_length = 0\n",
    "        beat_counter = 0\n",
    "        counters = {'S1':0,'S2':0}\n",
    "        i = 0\n",
    "        while i<count_vec.shape[0]:\n",
    "            stop = i \n",
    "            next_stop = i\n",
    "            while stop < count_vec.shape[0] and next_stop-stop<0.050*self.fs:\n",
    "                stop = next_stop\n",
    "                #on calcule la longueur du pic\n",
    "                while stop<count_vec.shape[0] and beat_mask[stop]:\n",
    "                    stop+=1\n",
    "\n",
    "                #on calcule l'écart avec le pic suivant, pour voir s'il s'agit du même pic\n",
    "                next_stop = stop\n",
    "                while next_stop<count_vec.shape[0] and not beat_mask[next_stop]:\n",
    "                    next_stop +=1\n",
    "                \n",
    "            #un battement long est un battement S1\n",
    "            if stop-i>mean_beat_length:\n",
    "                segmentation = np.concatenate((segmentation,np.repeat('S1',stop-i)))\n",
    "                counters['S1'] += 1\n",
    "                current_seq = {'beat':'S1','passive':'systole'}\n",
    "                \n",
    "            #un battement court est un battement S2\n",
    "            else:\n",
    "                segmentation = np.concatenate((segmentation,np.repeat('S2',stop-i)))\n",
    "                counters['S2'] += 1\n",
    "                current_seq = {'beat':'S2','passive':'diastole'}\n",
    "                \n",
    "            #update beat length and parser\n",
    "            mean_beat_length = np.average([stop-i,mean_beat_length],weights=[1,beat_counter])\n",
    "            beat_counter += 1\n",
    "            i = stop\n",
    "                \n",
    "            while stop<count_vec.shape[0] and systole_mask[stop]:\n",
    "                stop+=1\n",
    "                \n",
    "            segmentation = np.concatenate((segmentation,np.repeat(current_seq['passive'],stop-i)))\n",
    "            #update parser\n",
    "            i = stop\n",
    "        \n",
    "        #on supprime tout ce qui est avant la première phase S1: \n",
    "        j = 0 \n",
    "        while segmentation[j]!='S1':\n",
    "            j += 1\n",
    "            \n",
    "        return segmentation[j:],E[j:],j,counters\n",
    "    \n",
    "    \n",
    "    def timeLengths(self,segmentation,phase):\n",
    "        res=[]\n",
    "        start=0\n",
    "        while start<segmentation.shape[0]:\n",
    "            i = start\n",
    "            while i<segmentation.shape[0] and segmentation[i]==phase:\n",
    "                i+=1\n",
    "            res.append(i - start)\n",
    "            start = i + 1\n",
    "        res = np.array(res)[res!=0]\n",
    "        if res.shape[0] == 0:\n",
    "            return 0,0,0\n",
    "        return np.mean(res),np.std(res),res.shape[0]\n",
    "    \n",
    "    def timeFeatures(self,segmentation):\n",
    "        sysRR = [] #rapport durée systole/durée battement\n",
    "        diaRR = [] #rapport durée diastole/durée battement\n",
    "        sysdia = [] # rapport durée systole/durée diastole\n",
    "        start = 0\n",
    "        bound = len(segmentation)\n",
    "        #on commence sur la première phase S1\n",
    "        while segmentation[start]!='S1':\n",
    "            start+=1\n",
    "        #on parcourt toute la segmentation\n",
    "        while start<bound:\n",
    "            parser = start\n",
    "            # on attend d'arriver à la première systole\n",
    "            while parser<bound and segmentation[parser]!='systole':\n",
    "                parser += 1\n",
    "            psys = parser\n",
    "            # on mesure la taille de la systole\n",
    "            while psys<bound and segmentation[psys]=='systole':\n",
    "                psys += 1\n",
    "            systole_length = psys - parser\n",
    "            parser = psys\n",
    "            #on mesure la (les) diastole(s)\n",
    "            diastole_length_list=[]\n",
    "            while parser<bound and segmentation[parser]!='S1': #la fin du cycle est marquée par le début d'un nouveau S1\n",
    "                while parser<bound and segmentation[parser]=='S2':#un S2 précède toujours une diastole\n",
    "                    parser+=1\n",
    "                pd = parser\n",
    "                while pd<bound and segmentation[pd]=='diastole': #on mesure la diastole\n",
    "                    pd += 1\n",
    "                diastole_length_list.append(pd-parser)\n",
    "                parser = pd\n",
    "                \n",
    "            #grandeurs extraites\n",
    "            beat_length = parser - start\n",
    "            sysRR.append(systole_length/beat_length)\n",
    "            diaRR.append(sum(diastole_length_list)/beat_length)\n",
    "            if sum(diastole_length_list)!=0:\n",
    "                sysdia.append(systole_length/sum(diastole_length_list))\n",
    "            #update start\n",
    "            start = parser\n",
    "        sysRR = np.array(sysRR)\n",
    "        diaRR = np.array(diaRR)\n",
    "        sysdia = np.array(sysdia)\n",
    "        \n",
    "        return np.nan_to_num([np.mean(sysRR),np.std(sysRR),np.mean(diaRR),np.std(diaRR),np.mean(sysdia),np.std(sysdia)])\n",
    "   \n",
    "    def computeBorne(self,segmentation):\n",
    "        borne = []\n",
    "        \n",
    "        for i in range(len(segmentation)-1):\n",
    "            if segmentation[i-1] != segmentation[i]:\n",
    "                borne.append(i)\n",
    "        return borne \n",
    "    \n",
    "    def moments(self,segmentation,borne,phase,decalage):\n",
    "        phase_skew = []\n",
    "        phase_kurt = []\n",
    "        sig = self.signal[decalage:]\n",
    "        for i in range(0,len(borne)-1):\n",
    "            if segmentation[borne[i]] == phase:\n",
    "                interest_signal = sig[borne[i]:borne[i+1]]\n",
    "                phase_skew.append(skew(interest_signal))\n",
    "                phase_kurt.append(kurtosis(interest_signal))\n",
    "        if len(phase_skew) == 0:#cas d'erreur\n",
    "            return 0,0,0,0\n",
    "        return np.array([np.mean(phase_skew),np.std(phase_skew),np.mean(phase_kurt),np.std(phase_kurt)])\n",
    "    \n",
    "    def computeMFCC(self,borne,phase,decalage):\n",
    "        mfcc_matrix = np.zeros(10)\n",
    "        sig = self.signal[decalage:]\n",
    "        for i in range(0,len(borne)-1):\n",
    "            if segmentation[borne[i]] == phase:\n",
    "                interest_signal = sig[borne[i]:borne[i+1]]\n",
    "                mfccs = mfcc(interest_signal,sr=2000,n_mfcc=10)\n",
    "                mfcc_matrix = np.row_stack((mfcc_matrix,np.mean(mfccs,axis=0)))\n",
    "        if len(mfcc_matrix) == 0:#cas d'erreur\n",
    "            return np.zeros(10)\n",
    "        \n",
    "        return  np.mean(mfcc_matrix[1:],axis=1)\n",
    "        \n",
    "    def extractFeatures(self):\n",
    "        segmentation, E,decalage,counters = self.segmentation()\n",
    "        #ratio nombre de S1 par rapport au nombre de S2\n",
    "        if counters['S2']!=0:\n",
    "            features = np.array([counters['S1']/counters['S2']])\n",
    "        else: #cas d'erreur\n",
    "            features = np.zeros(1)\n",
    "        \n",
    "        #time features\n",
    "        to_compute_ratio = []\n",
    "        for phase in phases_list:\n",
    "            mean,std,last = self.timeLengths(segmentation,phase)\n",
    "            if phase == 'systole' or phase == 'diastole': #on aura en premier la systole \n",
    "                to_compute_ratio.append(last)\n",
    "            features = np.append(features,[mean,std])\n",
    "        features = np.append(features,np.nan_to_num(to_compute_ratio[0]/to_compute_ratio[1])) #durée de la systole sur la diastole\n",
    "        mean_sysRR, std_sysRR,mean_diaRR, std_diaRR,mean_sysdia, std_sysdia = self.timeFeatures(segmentation)\n",
    "        features = np.concatenate((features,[mean_sysRR, std_sysRR,mean_diaRR, std_diaRR,mean_sysdia, std_sysdia]))\n",
    "        \n",
    "        #calcul des bornes    \n",
    "        p_S1 = []\n",
    "        p_S2 = []\n",
    "        borne = self.computeBorne(segmentation)\n",
    "        \n",
    "        #Skewness et Kurtosis\n",
    "        for phase in phases_list:\n",
    "            features = np.concatenate((features,self.moments(segmentation,borne,phase,decalage)))\n",
    "        \n",
    "        #mean12 feature\n",
    "        features = np.append(features,max(features[2],features[5]))\n",
    "        \n",
    "        #Rapport d'amplitudes\n",
    "        sig = self.signal[decalage:]\n",
    "        amp_sys_s1 = []\n",
    "        amp_dia_s2 = []\n",
    "        \n",
    "        for i in range(0,len(borne)-2):\n",
    "            if (segmentation[borne[i]] == 'S1') & (segmentation[borne[i+1]] == 'systole') & (borne[i+2] != 0):\n",
    "                amp_sys_s1.append(max(sig[borne[i+1]:borne[i+2]]) - min(sig[borne[i+1]:borne[i+2]])/max(sig[borne[i]:borne[i+1]]) - min(sig[borne[i]:borne[i+1]]))\n",
    "            else :\n",
    "                pass\n",
    "        for i in range(0,len(borne)-2):\n",
    "            if (segmentation[borne[i]] == 'S2') & (segmentation[borne[i+1]] == 'diastole') & (borne[i+2] != 0):\n",
    "                amp_dia_s2.append(max(sig[borne[i+1]:borne[i+2]]) - min(sig[borne[i+1]:borne[i+2]])/max(sig[borne[i]:borne[i+1]]) - min(sig[borne[i]:borne[i+1]]) )\n",
    "            else :\n",
    "                pass\n",
    "        if len(amp_sys_s1)==0:\n",
    "            features = np.append(features,[0,0])\n",
    "        else: \n",
    "            features = np.append(features,[np.mean(amp_sys_s1),np.std(amp_sys_s1)])\n",
    "        if len(amp_dia_s2)==0:\n",
    "            features = np.append(features,[0,0])\n",
    "        else:\n",
    "            features = np.append(features,[np.mean(amp_dia_s2),np.std(amp_dia_s2)])\n",
    "\n",
    "        #spectral features\n",
    "        frequency_bands = np.array([0,25,40,60,90,120,160,250,400])\n",
    "        freqs, psd = welch(self.signal[decalage:],fs=2000)\n",
    "        freqs, psd = np.asarray(freqs),np.asarray(psd)\n",
    "        band_spectral_power = []\n",
    "        for i in range(frequency_bands.shape[0]-1):\n",
    "            mask = [freqs<frequency_bands[i+1]] and [freqs>frequency_bands[i]]\n",
    "            band_spectral_power.append(np.sum(psd[mask]))\n",
    "        features = np.append(features,band_spectral_power)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie va nous permettre de nous familiariser avec nos features à travers des représentations graphiques. Nous proposons une illustration de notre segmentation pour un signal type ainsi que des plots présentant les features que nous venons de construire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_length = 10000\n",
    "sample = train[87][:time_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcg = PCG(sample)\n",
    "segmentation,E,lag,c = pcg.segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "l = time_length-lag\n",
    "x = np.linspace(0,l/2000,l)\n",
    "plt.fill_between(x,0,1,where = segmentation=='S1',facecolor='r',alpha=.3,label='S1')\n",
    "plt.fill_between(x,0,1,where = segmentation=='S2',facecolor='r',alpha=.1,label='S2')\n",
    "plt.fill_between(x,0,1,where = segmentation=='systole',facecolor='b',alpha=.3,label='systole')\n",
    "plt.fill_between(x,0,1,where = segmentation=='diastole',facecolor='b',alpha=.1,label='diastole')\n",
    "\n",
    "plt.plot(x,E/np.max(E),'k',label='signal')\n",
    "plt.legend()\n",
    "plt.xlabel('temps (s)')\n",
    "plt.ylabel('amplitude normalisée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.zeros(20)\n",
    "for i in range(train.shape[0]):\n",
    "    sample = train[i]\n",
    "    pcg = PCG(sample)\n",
    "    segmentation,\n",
    "    mfccs = mfcc(sample,n_mfcc=20,sr=2000)\n",
    "    features = np.row_stack((features,np.min(mfccs,axis=1)))\n",
    "features = features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,20))\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(10,2,i+1)\n",
    "    tho = features[:,i]\n",
    "    ax.hist(tho[labels==1],density=True,alpha=0.5,color='b')\n",
    "    ax.hist(tho[labels==-1],density=True,alpha=0.5,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des features\n",
    "L'analyse de feature va nous permettre de déterminer le lien entre les variables que nous avons réussi à dégager et l'anomalie cardiaque. Nous proposerons également une comparaison graphique des variables entre des coeurs qui battent de façon normal et de façon anormale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sample = train[408]\n",
    "pcg = PCG(sample)\n",
    "ft = pcg.extractFeatures()\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.zeros(45)\n",
    "final_labels = []\n",
    "for i in range(n_samples):\n",
    "    try:\n",
    "        sample = train[i]\n",
    "        pcg = PCG(sample)\n",
    "        ft = pcg.extractFeatures()\n",
    "        X = np.row_stack((X,ft))\n",
    "        final_labels.append(labels[i])\n",
    "        \n",
    "    except:\n",
    "        print('il y a une erreur avec le sample %d'%i)\n",
    "X = X[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('features.npy',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,35))\n",
    "X = np.asarray(X)\n",
    "final_labels = np.array(final_labels)\n",
    "for i in range(51):\n",
    "    ax = fig.add_subplot(17,3,i+1)\n",
    "    ft = X[:,i]\n",
    "    #print(ft)\n",
    "    normalft = ft[final_labels==1]\n",
    "    anormalft = ft[final_labels==-1]\n",
    "    ax.hist(normalft,density=True,color='b',alpha=0.5)\n",
    "    ax.hist(anormalft,density=True,color='r',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convertion en Df au cas où\n",
    "X = np.column_stack((t1,t2,t12,t21,m12,ps1,ps2,good))\n",
    "X_df = pd.DataFrame(X, columns=[\"t1\",\"t2\",\"t12\",\"t21\",\"mean12\",\"S1\",\"S2\",\"Normal\"])\n",
    "X_df[\"Normal\"] = X_df[\"Normal\"].map({0 : \"anormal\", 1 : \"normal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Matrice de corrélation\n",
    "sns.heatmap(X_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Violin plot per variable\n",
    "features_list = X_df.columns[:-1]\n",
    "for i in features_list:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = sns.violinplot(x = [i]*len(X_df), y=X_df[i],hue=X_df[\"Normal\"], palette=\"Set2\", split = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Dans cette partie, nous allons appliquer des algorithmes de Machine Learning à nos features afin de détecter si un coeur est malade ou non\n",
    "Dans la mesure où notre problème est un cas de classification, nous avons considéré plusieurs modèles. Nos modèles benchmark sont le modèle qui prédit qu'un coeur est toujours normal ainsi que le modèle Logit. Par la suite, nous partirons d'un Random Forest et d'un SVM afin d'élaborer un modèle prédictif optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X_red = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10,max_depth=2)\n",
    "svm = SVC(C=100,gamma=100)\n",
    "nn = MLPClassifier()\n",
    "logit = LogisticRegression()\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,final_labels,test_size = 0.5)\n",
    "for clf in [logit,rf,svm]:\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    print(confusion_matrix(clf.predict(xtest),ytest)/len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(final_labels==1)/len(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = np.column_stack((t1,t2,t12,t21,ps1,ps2,m12))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_=y = X_df.loc[:,\"Normal\"].values\n",
    "labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logit\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_, test_size=0.20)\n",
    "logit = LogisticRegression()\n",
    "result = logit.fit(X_train, y_train)\n",
    "predictions = logit.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_, test_size=0.20)\n",
    "RF =  RandomForestClassifier(n_estimators=150, max_depth=7, random_state=0)\n",
    "result = RF.fit(X_train, y_train)\n",
    "predictions = RF.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_, test_size=0.20)\n",
    "clf = SVC()\n",
    "result = clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "print(cross_val_score(svm,features,labels_,cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RF\n",
    "param_grid = {'n_estimators': [100, 150, 200, 250, 300],'max_depth': [2, 4, 6, 7]}\n",
    "grid_RF = GridSearchCV(RF, param_grid, cv=10)\n",
    "grid_RF.fit(X_train, y_train)\n",
    "grid_RF. best_estimator_\n",
    "grid_RF. best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pour un SVM\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5, 1e-6],'C': [1, 10, 100, 1000]}]\n",
    "print(\"# Tuning hyper-parameters for %s\" % 'precision')\n",
    "print()\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Bof  \n",
    "Critiques ?  \n",
    "Ouverture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bibliographie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moukadem, A. (2011). Segmentation et classification des signaux non-stationnaires: application au traitement des sons cardiaque et à l'aide au diagnostic (Doctoral dissertation, Université de Haute Alsace-Mulhouse).\n",
    "\n",
    "Liang, H., Lukkarinen, S., & Hartimo, I. (1997, September). Heart sound segmentation algorithm based on heart sound envelogram. In Computers in Cardiology 1997 (pp. 105-108). IEEE.\n",
    "\n",
    "Safara, F., Doraisamy, S., Azman, A., Jantan, A., & Ramaiah, A. R. A. (2013). Multi-level basis selection of wavelet packet decomposition tree for heart sound classification. Computers in biology and medicine, 43(10), 1407-1414.\n",
    "\n",
    "Singh, M., & Cheema, A. (2013). Heart sounds classification using feature extraction of phonocardiography signal. International Journal of Computer Applications, 77(4).\n",
    "\n",
    "Atbi, A., & Debbal, S. M. (2013). Segmentation of pathological signals phonocardiogram by using the Shannon energy envelogram. AJCM, 2(1), 1-14."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
